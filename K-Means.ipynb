{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.cluster import KMeans\n",
    "from functools import reduce\n",
    "from build_sentence_corp import extract_sent\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_size = 300\n",
    "embedding_path = 'models/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(sentence_dir):\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    vocab_count = {}\n",
    "    for sent in sentence_dir:\n",
    "        for word in sent.split():\n",
    "            word = word.lower().translate(table).strip()\n",
    "            if word in vocab_count:\n",
    "                vocab_count[word] += 1\n",
    "            else:\n",
    "                vocab_count[word] = 1\n",
    "        \n",
    "    vocab = set()\n",
    "    k = 10    \n",
    "    for key, val in vocab_count.items():\n",
    "        if val >= k and key not in stopwords.words('english'):\n",
    "            vocab.add(key)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent(path, regex):\n",
    "    review_dir = []\n",
    "    for file in os.listdir(path):\n",
    "        with open(path+file) as f:\n",
    "            txt = f.read()\n",
    "            reviews = re.findall(regex, txt)\n",
    "        review_dir += reviews\n",
    "    \n",
    "    sentence_dir = []\n",
    "    for review in review_dir:\n",
    "        sentences = nltk.sent_tokenize(review)\n",
    "        sentence_dir += sentences\n",
    "        \n",
    "    return sentence_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeT(words):\n",
    "    matrix = np.empty((0, embedding_vector_size))\n",
    "    for w in words:\n",
    "        try:\n",
    "            matrix = np.vstack((matrix, embedding[w]))\n",
    "        except:\n",
    "            pass\n",
    "    print(matrix.shape)\n",
    "    kmeans = KMeans(n_clusters=14, random_state=0).fit(matrix)\n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = './data/TripAdvisor/Texts/'\n",
    "regex = r\"<Content>(.*)\\n<Date>\" \n",
    "sentence_dir = extract_sent(text_path, regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon  Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/AmazonReviews/laptops/'\n",
    "reviews = []\n",
    "for file in os.listdir(path):\n",
    "    with open(path+file) as f:\n",
    "        data = json.load(f)\n",
    "        for review in data['Reviews']:\n",
    "            reviews.append(review['Content'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'ve had the S7-391 with 4Gb RAM and a 256 GB SSD for about 6 months, having paid a lot more for it when it first came out. I can recommend it, but only if you understand the limitations and problems with the machine. Here\\'s a quick summary of my experience to date:GoodIt\\'s very thin and very light. The screen has excellent resolution and reasonably responsive touch capabilities. Since it turns itself on when you open it, you need to actually shut it down to (carefully) clean the screen. it uses an SSD and never shuts itself off unless you force it to shut down, it comes up quite fast and is ready to go. It\\'s even fast on a cold boot, like most SSD systems.That\\'s mostly it for the good.Not GoodUntil recently, the WiFi would just stop connecting and I\\'d need to manually reconnect to my router or an AP at work. Bluetooth would also get wiggy and stop communicating, particularly after a modest pause in activity. This was such a regular \"feature\" with the included BT mouse that I had to search out one that would work better. Never found one, though the Logitech BT travel mouse works more often than not.Battery life is also not good. Running the screen at around 80% gives me about three hours or a bit more. Dimming down to 50% gives me five hours and change, but given the  level of eye strain it feels more like 8 hours. On the other hand, the brick isn\\'t too  large and packs easily into a briefcase of bag.The BadMan but I hate - HATE - typing on  this thing. Sure, all ultrabooks/Macbook Air have \"Chiclet\" type keyboards, and maybe the Aspire isn\\'t particularly worse than most, but it sure feels that way. I am normally an extraordinarily fast and accurate typist, but on this machine I may be better off using hunt & peck. Trying to put any speed on my typing results in doubled letters every other word or so, or dropped letters. The ratio is about 4 doubled letters for each dropped letter. Only by slowing way, way down can I get an entire sentence out without correcting it, something that just doesn\\'t happen when I use the standard keyboard on my desktop tower.As somebody who writes and edits a great deal, I have actually decided to get some other machine, perhaps a Lenovo, and just figure the Aspire is a sunk cost. But if thin and light is more important to you than keyboarding, then it\\'s worth considering.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dir = []\n",
    "for review in reviews:\n",
    "    if review:\n",
    "        sentences = nltk.sent_tokenize(review)\n",
    "        sentence_dir += sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323984"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = generate_vocab(sentence_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11352"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9300, 300)\n"
     ]
    }
   ],
   "source": [
    "matrix = np.empty((0, embedding_vector_size))\n",
    "for w in words:\n",
    "    try:\n",
    "        matrix = np.vstack((matrix, embedding[w]))\n",
    "    except:\n",
    "        pass\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open('amazon_laptop_matrix.pickle', 'wb')\n",
    "pickle.dump(matrix, a)\n",
    "a.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9300, 300)\n"
     ]
    }
   ],
   "source": [
    "kmeans = initializeT(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_T = open('T_matrix.pickle', 'wb')\n",
    "pickle.dump(kmeans.cluster_centers_, pickle_T)\n",
    "pickle_T.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('T_matrix.pickle', 'rb')\n",
    "centers = pickle.load(pickle_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
